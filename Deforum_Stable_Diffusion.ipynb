{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c442uQJ_gUgy"
      },
      "source": [
        "# **Deforum Stable Diffusion**\n",
        "[Stable Diffusion](https://github.com/CompVis/stable-diffusion) by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer and the [Stability.ai](https://stability.ai/) Team\n",
        "\n",
        "Notebook by [deforum](https://twitter.com/deforum_art)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g-f7cQmf2Nt",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown **NVIDIA GPU**\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRNl2mfepEIe",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown **Setup Environment**\n",
        "\n",
        "setup_environment = True #@param {type:\"boolean\"}\n",
        "\n",
        "if setup_environment:\n",
        "  %pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "  %pip install omegaconf==2.1.1 einops==0.3.0 pytorch-lightning==1.4.2 torchmetrics==0.6.0 torchtext==0.2.3 transformers==4.19.2 kornia==0.6\n",
        "  !git clone https://github.com/deforum/stable-diffusion\n",
        "  %pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
        "  %pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "  %pip install git+https://github.com/deforum/k-diffusion/\n",
        "  print(\"Runtime > Restart Runtime\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown **Python Definitions**\n",
        "import json\n",
        "from IPython import display\n",
        "\n",
        "import sys, os\n",
        "import argparse, glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm, trange\n",
        "from itertools import islice\n",
        "from einops import rearrange, repeat\n",
        "from torchvision.utils import make_grid\n",
        "import time\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch import autocast\n",
        "from contextlib import contextmanager, nullcontext\n",
        "\n",
        "sys.path.append('./stable-diffusion/')\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "\n",
        "import k_diffusion as K\n",
        "import accelerate\n",
        "\n",
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "def get_output_folder(output_path,batch_folder=None):\n",
        "    yearMonth = time.strftime('%Y-%m/')\n",
        "    out_path = output_path+\"/\"+yearMonth\n",
        "    if batch_folder != \"\":\n",
        "        out_path += batch_folder\n",
        "        if out_path[-1] != \"/\":\n",
        "            out_path += \"/\"\n",
        "    os.makedirs(out_path, exist_ok=True)\n",
        "    return out_path\n",
        "\n",
        "def load_img(path):\n",
        "    image = Image.open(path).convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float16) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "class CFGDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, cond])\n",
        "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
        "        return uncond + (cond - uncond) * cond_scale\n",
        "\n",
        "def run(params):\n",
        "\n",
        "    # timestring\n",
        "    timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "\n",
        "    # outpath\n",
        "    os.makedirs(params[\"outdir\"], exist_ok=True)\n",
        "    outpath = params[\"outdir\"]\n",
        "\n",
        "    # random seed\n",
        "    if params[\"seed\"] == -1:\n",
        "        local_seed = np.random.randint(0,4294967295)\n",
        "    else:\n",
        "        local_seed = params[\"seed\"]\n",
        "\n",
        "    # load settings\n",
        "\n",
        "    # save/append settings\n",
        "    if params[\"save_settings\"] and params[\"filename\"] is None:\n",
        "        filename = f\"{timestring}_settings.txt\"\n",
        "        assert not os.path.isfile(f\"{outpath}{filename}\")\n",
        "        params[\"filename\"] = f\"{timestring}_settings.txt\"\n",
        "        params[\"batch_seeds\"] = [local_seed]\n",
        "        with open(f\"{outpath}{filename}\", \"w+\") as f:\n",
        "            json.dump(params, f, ensure_ascii=False, indent=4)\n",
        "    elif params[\"save_settings\"] and params[\"filename\"] is not None:\n",
        "        filename = params[\"filename\"]\n",
        "        with open(f\"{outpath}{filename}\") as f:\n",
        "            params = json.load(f)\n",
        "        params[\"batch_seeds\"] += [local_seed]\n",
        "        with open(f\"{outpath}{filename}\", \"w+\") as f:\n",
        "            json.dump(params, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    # batch size\n",
        "    batch_size = params[\"n_samples\"]\n",
        "    n_rows = params[\"n_rows\"] if params[\"n_rows\"] > 0 else batch_size\n",
        "\n",
        "    # make prompts\n",
        "    if not params[\"from_file\"]:\n",
        "        prompt = params[\"prompt\"]\n",
        "        assert prompt is not None\n",
        "        data = [batch_size * [prompt]]\n",
        "    else:\n",
        "        infile = params[\"from_file\"]\n",
        "        print(f\"reading prompts from {infile}\")\n",
        "        with open(infile, \"r\") as f:\n",
        "            data = f.read().splitlines()\n",
        "            data = list(chunk(data, batch_size))\n",
        "\n",
        "    # run\n",
        "    precision_scope = autocast if params[\"precision\"]==\"autocast\" else nullcontext\n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            with model.ema_scope():\n",
        "\n",
        "                # start timer\n",
        "                tic = time.time()\n",
        "\n",
        "                # start\n",
        "                for prompts in data:\n",
        "\n",
        "                    # conditional/unconditional\n",
        "                    uc = None\n",
        "                    if params[\"scale\"] != 1.0:\n",
        "                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                    if isinstance(prompts, tuple):\n",
        "                        prompts = list(prompts)\n",
        "                    c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "                    # shape\n",
        "                    shape = [params[\"C\"], params[\"H\"] // params[\"f\"], params[\"W\"] // params[\"f\"]]\n",
        "\n",
        "                    # device\n",
        "                    accelerator = accelerate.Accelerator()\n",
        "                    device = accelerator.device\n",
        "                    seeds = torch.randint(-2 ** 63, 2 ** 63 - 1, [accelerator.num_processes])\n",
        "                    torch.manual_seed(seeds[accelerator.process_index].item())\n",
        "\n",
        "                    # seed\n",
        "                    seed_everything(local_seed)\n",
        "\n",
        "                    # k samplers\n",
        "                    if params[\"sampler\"] in [\"klms\",\"dpm2\",\"dpm2_ancestral\",\"heun\",\"euler\",\"euler_ancestral\"]:\n",
        "\n",
        "                        model_wrap = K.external.CompVisDenoiser(model)\n",
        "                        sigma_min, sigma_max = model_wrap.sigmas[0].item(), model_wrap.sigmas[-1].item()\n",
        "\n",
        "                        sigmas = model_wrap.get_sigmas(params[\"steps\"])\n",
        "                        torch.manual_seed(local_seed)\n",
        "                        x = torch.randn([params[\"n_samples\"], *shape], device=device) * sigmas[0]\n",
        "                        model_wrap_cfg = CFGDenoiser(model_wrap)\n",
        "                        extra_args = {'cond': c, 'uncond': uc, 'cond_scale': params[\"scale\"]}\n",
        "\n",
        "                        if params[\"sampler\"]==\"klms\":\n",
        "                            samples = K.sampling.sample_lms(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=not accelerator.is_main_process)\n",
        "                        elif params[\"sampler\"]==\"dpm2\":\n",
        "                            samples = K.sampling.sample_dpm_2(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=not accelerator.is_main_process)\n",
        "                        elif params[\"sampler\"]==\"dpm2_ancestral\":\n",
        "                            samples = K.sampling.sample_dpm_2_ancestral(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=not accelerator.is_main_process)\n",
        "                        elif params[\"sampler\"]==\"heun\":\n",
        "                            samples = K.sampling.sample_heun(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=not accelerator.is_main_process)\n",
        "                        elif params[\"sampler\"]==\"euler\":\n",
        "                            samples = K.sampling.sample_euler(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=not accelerator.is_main_process)\n",
        "                        elif params[\"sampler\"]==\"euler_ancestral\":\n",
        "                            samples = K.sampling.sample_euler_ancestral(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=not accelerator.is_main_process)\n",
        "                        \n",
        "                        x_samples = model.decode_first_stage(samples)\n",
        "                        x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                        x_samples = accelerator.gather(x_samples)\n",
        "\n",
        "                    # sd samplers\n",
        "                    if params[\"sampler\"] in [\"plms\",\"ddim\"]:\n",
        "\n",
        "                        # make samplers\n",
        "                        if params[\"sampler\"]==\"plms\":\n",
        "                            params[\"eta\"] = 0\n",
        "                            sampler = PLMSSampler(model)\n",
        "                        else:\n",
        "                            sampler = DDIMSampler(model)\n",
        "\n",
        "                        # initial image\n",
        "                        start_code = None\n",
        "                        if params[\"use_init\"]:\n",
        "\n",
        "                            assert os.path.isfile(params[\"init_image\"])\n",
        "                            init_image = load_img(params[\"init_image\"]).to(device)\n",
        "                            init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "                            init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
        "                            sampler.make_schedule(ddim_num_steps=params['steps'], ddim_eta=params['eta'], verbose=False)\n",
        "                            assert 0. <= params['strength'] <= 1., 'can only work with strength in [0.0, 1.0]'\n",
        "                            t_enc = int(params['strength'] * params['steps'])\n",
        "                            print(f\"target t_enc is {t_enc} steps\")\n",
        "\n",
        "                            # encode (scaled latent)\n",
        "                            z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
        "                            # decode it\n",
        "                            samples = sampler.decode(z_enc, c, t_enc,\n",
        "                                                     unconditional_guidance_scale=params['scale'],\n",
        "                                                     unconditional_conditioning=uc)\n",
        "                            x_samples = model.decode_first_stage(samples)\n",
        "                            x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "                        else:\n",
        "\n",
        "                            if params[\"fixed_code\"]:\n",
        "                                start_code = torch.randn([params[\"n_samples\"], params[\"C\"], params[\"H\"] // params[\"f\"], params[\"W\"] // params[\"f\"]], device=device)\n",
        "                            samples, _ = sampler.sample(S=params[\"steps\"],\n",
        "                                                             conditioning=c,\n",
        "                                                             batch_size=params[\"n_samples\"],\n",
        "                                                             shape=shape,\n",
        "                                                             verbose=False,\n",
        "                                                             unconditional_guidance_scale=params[\"scale\"],\n",
        "                                                             unconditional_conditioning=uc,\n",
        "                                                             eta=params[\"eta\"],\n",
        "                                                             x_T=start_code)\n",
        "\n",
        "                            x_samples = model.decode_first_stage(samples)\n",
        "                            x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                            x_samples = x_samples\n",
        "\n",
        "\n",
        "                    # save samples\n",
        "                    if params[\"display_samples\"] or params[\"save_samples\"]:\n",
        "                        for count, x_sample in enumerate(x_samples):\n",
        "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                            if params[\"display_samples\"]:\n",
        "                                display.display(Image.fromarray(x_sample.astype(np.uint8)))\n",
        "                            if params[\"save_samples\"]:\n",
        "                                Image.fromarray(x_sample.astype(np.uint8)).save(\n",
        "                                    os.path.join(outpath, f\"{timestring}_{count:02}_{local_seed}.png\"))\n",
        "\n",
        "                    # save grid\n",
        "                    if params[\"display_grid\"] or params[\"save_grid\"]:\n",
        "                        grid = torch.stack([x_samples], 0)\n",
        "                        grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "                        grid = make_grid(grid, nrow=n_rows)\n",
        "                        grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "                        if params[\"display_grid\"]:\n",
        "                            display.display(Image.fromarray(grid.astype(np.uint8)))\n",
        "                        if params[\"save_grid\"]:\n",
        "                            Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'{timestring}_{local_seed}_grid.png'))\n",
        "                \n",
        "                # stop timer\n",
        "                toc = time.time()                       \n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "81qmVZbrm4uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzU1bmrigJJB",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown **Local Path Variables**\n",
        "print(\"Local Path Variables:\\n\")\n",
        "\n",
        "models_path = \"/content/models\" #@param {type:\"string\"}\n",
        "output_path = \"/content/output\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown **Google Drive Path Variables (Optional)**\n",
        "mount_google_drive = True #@param {type:\"boolean\"}\n",
        "force_remount = False\n",
        "\n",
        "if mount_google_drive:\n",
        "  from google.colab import drive\n",
        "  try:\n",
        "    drive_path = \"/content/drive\"\n",
        "    drive.mount(drive_path,force_remount=force_remount)\n",
        "    models_path = \"/content/drive/MyDrive/AI/models\" #@param {type:\"string\"}\n",
        "    output_path = \"/content/drive/MyDrive/AI/StableDiffusion\" #@param {type:\"string\"}\n",
        "  except:\n",
        "    print(\"...error mounting drive or with drive path variables\")\n",
        "    print(\"...reverting to default path variables\")\n",
        "    models_path = \"/content/models\"\n",
        "    output_path = \"/content/output\"\n",
        "\n",
        "!mkdir -p $models_path\n",
        "!mkdir -p $output_path\n",
        "\n",
        "print(f\"models_path: {models_path}\")\n",
        "print(f\"output_path: {output_path}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "#@markdown **Select Model**\n",
        "print(\"\\nSelect Model:\\n\")\n",
        "\n",
        "model_config = \"v1-inference.yaml\" #@param [\"v1-inference.yaml\",\"custom\"]\n",
        "model_checkpoint =  \"sd-v1-3-full-ema.ckpt\" #@param [\"sd-v1-3-full-ema.ckpt\",\"custom\"]\n",
        "check_sha256 = True #@param {type:\"boolean\"}\n",
        "\n",
        "model_map = {\n",
        "    'sd-v1-3-full-ema.ckpt': {'downloaded': False, 'sha256': '54632c6e8a36eecae65e36cb0595fab314e1a1545a65209f24fde221a8d4b2ca', 'link': ['https://drinkordiecdn.lol/sd-v1-3-full-ema.ckpt'] },\n",
        "  }\n",
        "\n",
        "def download_model(model_checkpoint):\n",
        "  download_link = model_map[model_checkpoint][\"link\"][0]\n",
        "  print(f\"!wget -O {models_path}/{model_checkpoint} {download_link}\")\n",
        "  !wget -O $models_path/$model_checkpoint $download_link\n",
        "  return\n",
        "\n",
        "# config path\n",
        "if os.path.exists(models_path+'/'+model_config):\n",
        "  print(f\"{models_path+'/'+model_config} exists\")\n",
        "else:\n",
        "  print(\"cp ./stable-diffusion/configs/stable-diffusion/v1-inference.yaml $models_path/.\")\n",
        "  !cp ./stable-diffusion/configs/stable-diffusion/v1-inference.yaml $models_path/.\n",
        "\n",
        "# checkpoint path or download\n",
        "if os.path.exists(models_path+'/'+model_checkpoint):\n",
        "  print(f\"{models_path+'/'+model_checkpoint} exists\")\n",
        "else:\n",
        "  print(\"...downloading checkpoint\")\n",
        "  download_model(model_checkpoint)\n",
        "\n",
        "if check_sha256:\n",
        "  import hashlib\n",
        "  print(\"...checking sha256\")\n",
        "  with open(models_path+'/'+model_checkpoint, \"rb\") as f:\n",
        "    bytes = f.read() \n",
        "    hash = hashlib.sha256(bytes).hexdigest()\n",
        "    del bytes\n",
        "  assert model_map[model_checkpoint][\"sha256\"] == hash\n",
        "\n",
        "config = models_path+'/'+model_config\n",
        "ckpt = models_path+'/'+model_checkpoint\n",
        "\n",
        "print(f\"config: {config}\")\n",
        "print(f\"ckpt: {ckpt}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "#@markdown **Load Model**\n",
        "print(\"\\nLoad Model:\\n\")\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False, device='cuda'):\n",
        "    map_location = \"cuda\" #@param [\"cpu\", \"cuda\"]\n",
        "    print(f\"...loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=map_location)\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    #model.cuda()\n",
        "    model = model.half().to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "local_config = OmegaConf.load(f\"{config}\")\n",
        "model = load_model_from_config(local_config, f\"{ckpt}\")\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov3r4RD1tzsT"
      },
      "source": [
        "# **Run**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qH74gBWDd2oq"
      },
      "outputs": [],
      "source": [
        "def opt_params():\n",
        "  \n",
        "  #@markdown **Save & Display Settings**\n",
        "  batchdir = \"test\" #@param {type:\"string\"}\n",
        "  outdir = get_output_folder(output_path,batchdir)\n",
        "  save_settings = False #@param {type:\"boolean\"}\n",
        "  save_grid = True #@param {type:\"boolean\"}\n",
        "  display_grid = True #@param {type:\"boolean\"}\n",
        "  save_samples = True #@param {type:\"boolean\"}\n",
        "  display_samples = False #@param {type:\"boolean\"}\n",
        "\n",
        "  #@markdown **Prompt Settings**\n",
        "  seed = 1574552011 #@param\n",
        "  prompt = \"photorealistic painting portrait of a beautiful gorgeous glorious majestic young punjabi princess with headphones figurative liminal complex flat natural realism minimalism by kehinde wiley shadi ghadirian jimmy nelson oil on canvas cosmic levels shimmer pastel color \" #@param {type:\"string\"}\n",
        "  from_file = False #@param {type:\"boolean\"}\n",
        "\n",
        "  #@markdown **Image Settings**\n",
        "  n_samples = 1 #@param\n",
        "  n_rows = 1 #@param\n",
        "  W = 512 #@param\n",
        "  H = 768 #@param\n",
        "\n",
        "  #@markdown **Init Settings**\n",
        "  use_init = True #@param {type:\"boolean\"}\n",
        "  init_image = \"/content/drive/MyDrive/AI/StableDiffusion/20220815180851_0.png\" #@param {type:\"string\"}\n",
        "  strength = 0.1 #@param {type:\"number\"}\n",
        "  \n",
        "  #@markdown **Sampling Settings**\n",
        "  scale = 7 #@param\n",
        "  n_iter = 1 #@param\n",
        "  steps = 10 #@param\n",
        "  eta = 0.0 #@param\n",
        "  sampler = 'dpm2' #@param [\"klms\",\"dpm2\",\"dpm2_ancestral\",\"heun\",\"euler\",\"euler_ancestral\",\"plms\", \"ddim\"]\n",
        "  \n",
        "  #@markdown **Batch Settings**\n",
        "  n_batch = 1 #@param\n",
        "\n",
        "  #@markdown **no idea what this does**\n",
        "  precision = 'autocast' #@param [\"full\", \"autocast\"]\n",
        "  fixed_code = True #@param\n",
        "  C = 4 #@param\n",
        "  f = 8 #@param\n",
        "\n",
        "  return locals()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxx8BzxjiaXg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown **Run**\n",
        "params = opt_params()\n",
        "params[\"filename\"] = None\n",
        "for ii in range(params[\"n_batch\"]):\n",
        "  num = params[\"n_batch\"]\n",
        "  print(f\"run {ii+1} of {num}\")\n",
        "  run(params)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Deforum Stable Diffusion",
      "provenance": [],
      "private_outputs": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}