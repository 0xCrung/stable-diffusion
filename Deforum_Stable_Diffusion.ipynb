{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c442uQJ_gUgy"
      },
      "source": [
        "# **Deforum Stable Diffusion**\n",
        "[Stable Diffusion](https://github.com/CompVis/stable-diffusion) by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer and the [Stability.ai](https://stability.ai/) Team\n",
        "\n",
        "Notebook by [deforum](https://twitter.com/deforum_art)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g-f7cQmf2Nt",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown **GPU**\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRNl2mfepEIe",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown **Env Setup**\n",
        "\n",
        "#@markdown Runtime > Restart Runtime\n",
        "!pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install omegaconf==2.1.1 einops==0.3.0 pytorch-lightning==1.4.2 torchmetrics==0.6.0 torchtext==0.2.3 transformers==4.19.2 kornia==0.6\n",
        "!git clone https://github.com/deforum/stable-diffusion -b dev\n",
        "!pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
        "!pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzU1bmrigJJB",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown **Local Path Variables**\n",
        "models_path = \"/content/models\" #@param {type:\"string\"}\n",
        "output_path = \"/content/output\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown **Google Drive Path Variables (Optional)**\n",
        "mount_google_drive = True #@param {type:\"boolean\"}\n",
        "force_remount = False\n",
        "\n",
        "if mount_google_drive:\n",
        "  from google.colab import drive\n",
        "  try:\n",
        "    drive_path = \"/content/drive\"\n",
        "    drive.mount(drive_path,force_remount=force_remount)\n",
        "    models_path = \"/content/drive/MyDrive/AI/models\" #@param {type:\"string\"}\n",
        "    output_path = \"/content/drive/MyDrive/AI/StableDiffusion\" #@param {type:\"string\"}\n",
        "  except:\n",
        "    print(\"...error mounting drive or with drive path variables\")\n",
        "    print(\"...reverting to default path variables\")\n",
        "    models_path = \"/content/models\"\n",
        "    output_path = \"/content/output\"\n",
        "\n",
        "!mkdir -p $models_path\n",
        "!mkdir -p $output_path\n",
        "\n",
        "print(f\"models_path: {models_path}\")\n",
        "print(f\"output_path: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FVWjI1vclPO",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown **Python Definitions**\n",
        "import json\n",
        "from IPython import display\n",
        "#from tqdm.notebook import tqdm, trange\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "import sys, os\n",
        "import argparse, glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm, trange\n",
        "from itertools import islice\n",
        "from einops import rearrange, repeat\n",
        "from torchvision.utils import make_grid\n",
        "import time\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch import autocast\n",
        "from contextlib import contextmanager, nullcontext\n",
        "\n",
        "sys.path.append('./stable-diffusion/')\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "\n",
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "def get_output_folder(output_path,batch_folder=None):\n",
        "  yearMonth = time.strftime('%Y-%m/')\n",
        "  out_path = output_path+\"/\"+yearMonth\n",
        "  if batch_folder != \"\":\n",
        "    out_path += batch_folder\n",
        "    if out_path[-1] != \"/\":\n",
        "      out_path += \"/\"\n",
        "  os.makedirs(out_path, exist_ok=True)\n",
        "  return out_path\n",
        "\n",
        "def load_img(path):\n",
        "    image = Image.open(path).convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float16) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "def run(params):\n",
        "\n",
        "    # outpath\n",
        "    os.makedirs(params[\"outdir\"], exist_ok=True)\n",
        "    outpath = params[\"outdir\"]\n",
        "\n",
        "    # timestring\n",
        "    timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "\n",
        "    # random seed\n",
        "    if params[\"seed\"] == -1:\n",
        "      local_seed = np.random.randint(0,4294967295)\n",
        "    else:\n",
        "      local_seed = params[\"seed\"]\n",
        "\n",
        "    # save/append settings\n",
        "    if params[\"save_settings\"] and params[\"filename\"] is None:\n",
        "      filename = f\"{timestring}_settings.txt\"\n",
        "      assert not os.path.isfile(f\"{outpath}{filename}\")\n",
        "      params[\"filename\"] = f\"{timestring}_settings.txt\"\n",
        "      params[\"batch_seeds\"] = [local_seed]\n",
        "      with open(f\"{outpath}{filename}\", \"w+\") as f:\n",
        "        json.dump(params, f, ensure_ascii=False, indent=4)\n",
        "    elif params[\"save_settings\"] and params[\"filename\"] is not None:\n",
        "      filename = params[\"filename\"]\n",
        "      with open(f\"{outpath}{filename}\") as f:\n",
        "        params = json.load(f)\n",
        "      params[\"batch_seeds\"] += [local_seed]\n",
        "      with open(f\"{outpath}{filename}\", \"w+\") as f:\n",
        "        json.dump(params, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    # load settings\n",
        "\n",
        "    # seed\n",
        "    seed_everything(local_seed)\n",
        "\n",
        "    # plms\n",
        "    if params[\"plms\"]:\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "\n",
        "    batch_size = params[\"n_samples\"]\n",
        "    n_rows = params[\"n_rows\"] if params[\"n_rows\"] > 0 else batch_size\n",
        "    if not params[\"from_file\"]:\n",
        "        prompt = params[\"prompt\"]\n",
        "        assert prompt is not None\n",
        "        data = [batch_size * [prompt]]\n",
        "\n",
        "    else:\n",
        "        print(f\"reading prompts from {from_file}\")\n",
        "        with open(from_file, \"r\") as f:\n",
        "            data = f.read().splitlines()\n",
        "            data = list(chunk(data, batch_size))\n",
        "\n",
        "    #sample_path = os.path.join(outpath, \"samples\")\n",
        "    #os.makedirs(sample_path, exist_ok=True)\n",
        "    #base_count = len(os.listdir(sample_path))\n",
        "    base_count = 0\n",
        "    #grid_count = len(os.listdir(outpath)) - 1\n",
        "\n",
        "    start_code = None\n",
        "    \n",
        "    # init image\n",
        "    if params[\"use_init\"]:\n",
        "      assert os.path.isfile(params[\"init_image\"])\n",
        "      init_image = load_img(params[\"init_image\"]).to(device)\n",
        "      init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "      init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
        "\n",
        "      sampler.make_schedule(ddim_num_steps=params['ddim_steps'], ddim_eta=params['ddim_eta'], verbose=False)\n",
        "  \n",
        "      assert 0. <= params['strength'] <= 1., 'can only work with strength in [0.0, 1.0]'\n",
        "      t_enc = int(params['strength'] * params['ddim_steps'])\n",
        "      print(f\"target t_enc is {t_enc} steps\")\n",
        "\n",
        "    # no init image\n",
        "    else:\n",
        "      if params[\"fixed_code\"]:\n",
        "          start_code = torch.randn([params[\"n_samples\"], params[\"C\"], params[\"H\"] // params[\"f\"], params[\"W\"] // params[\"f\"]], device=device)\n",
        "\n",
        "    precision_scope = autocast if params[\"precision\"]==\"autocast\" else nullcontext\n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            with model.ema_scope():\n",
        "                tic = time.time()\n",
        "                all_samples = list()\n",
        "                #for n in trange(params[\"n_iter\"], desc=\"Sampling\"):\n",
        "                for n in range(params[\"n_iter\"]):\n",
        "                    #for prompts in tqdm(data, desc=\"data\"):\n",
        "                    for prompts in data:\n",
        "                        uc = None\n",
        "                        if params[\"scale\"] != 1.0:\n",
        "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                        if isinstance(prompts, tuple):\n",
        "                            prompts = list(prompts)\n",
        "                        c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "                        # no init image\n",
        "                        if not params['use_init']:\n",
        "                            shape = [params[\"C\"], params[\"H\"] // params[\"f\"], params[\"W\"] // params[\"f\"]]\n",
        "                            samples_ddim, _ = sampler.sample(S=params[\"ddim_steps\"],\n",
        "                                                            conditioning=c,\n",
        "                                                            batch_size=params[\"n_samples\"],\n",
        "                                                            shape=shape,\n",
        "                                                            verbose=False,\n",
        "                                                            unconditional_guidance_scale=params[\"scale\"],\n",
        "                                                            unconditional_conditioning=uc,\n",
        "                                                            eta=params[\"ddim_eta\"],\n",
        "                                                            x_T=start_code)\n",
        "\n",
        "                            x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "                            x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                            x_samples = x_samples_ddim\n",
        "\n",
        "                        # init image\n",
        "                        else:\n",
        "                          # encode (scaled latent)\n",
        "                          z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
        "                          # decode it\n",
        "                          samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=params['scale'],\n",
        "                                                   unconditional_conditioning=uc,)\n",
        "  \n",
        "                          x_samples = model.decode_first_stage(samples)\n",
        "                          x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                        \n",
        "                        for x_sample in x_samples:\n",
        "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                            if params[\"display_samples\"]:\n",
        "                                display.display(Image.fromarray(x_sample.astype(np.uint8)))\n",
        "                            if not params[\"skip_save\"]:\n",
        "                                Image.fromarray(x_sample.astype(np.uint8)).save(\n",
        "                                      os.path.join(outpath, f\"{timestring}_{base_count:02}.png\"))\n",
        "                            base_count += 1\n",
        "\n",
        "                        if not params[\"skip_grid\"]:\n",
        "                            all_samples.append(x_samples)\n",
        "\n",
        "                if not params[\"skip_grid\"]:\n",
        "                    # additionally, save as grid\n",
        "                    grid = torch.stack(all_samples, 0)\n",
        "                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "                    grid = make_grid(grid, nrow=n_rows)\n",
        "\n",
        "                    # to image\n",
        "                    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "                    if params[\"display_grid\"]:\n",
        "                        display.display(Image.fromarray(grid.astype(np.uint8)))\n",
        "                    Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'{timestring}_grid.png'))\n",
        "                    #grid_count += 1\n",
        "\n",
        "                toc = time.time()\n",
        "\n",
        "    #print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\" f\" \\nEnjoy.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CIUJ7lWI4v53"
      },
      "outputs": [],
      "source": [
        "#@markdown **Model Path Variables**\n",
        "# ask for the link\n",
        "download_link = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# config\n",
        "if os.path.exists(models_path+'/v1-inference.yaml'):\n",
        "  print(f\"{models_path+'/v1-inference.yaml'} exists\")\n",
        "else:\n",
        "  print(\"cp ./stable-diffusion/configs/stable-diffusion/v1-inference.yaml $models_path/.\")\n",
        "  !cp ./stable-diffusion/configs/stable-diffusion/v1-inference.yaml $models_path/.\n",
        "\n",
        "# weights \n",
        "if os.path.exists(models_path+'/v1-inference.ckpt'):\n",
        "  print(f\"{models_path+'/v1-inference.ckpt'} exists\")\n",
        "else:\n",
        "  print(f\"!wget -O $models_path/v1-inference.ckpt {download_link}\")\n",
        "  !wget -O $models_path/v1-inference.ckpt $download_link\n",
        "\n",
        "config = models_path+'/v1-inference.yaml'\n",
        "ckpt = models_path+'/v1-inference.ckpt'\n",
        "\n",
        "print(f\"config: {config}\")\n",
        "print(f\"ckpt: {ckpt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJiMgz_96nr3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown **Load Stable Diffusion**\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False, device='cuda'):\n",
        "    map_location = \"cuda\" #@param [\"cpu\", \"cuda\"]\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=map_location)\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    #model.cuda()\n",
        "    model = model.half().to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "local_config = OmegaConf.load(f\"{config}\")\n",
        "model = load_model_from_config(local_config, f\"{ckpt}\")\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov3r4RD1tzsT"
      },
      "source": [
        "# **Run**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qH74gBWDd2oq"
      },
      "outputs": [],
      "source": [
        "def opt_params():\n",
        "  \n",
        "  #@markdown **Save & Display Settings**\n",
        "  batchdir = \"new\" #@param {type:\"string\"}\n",
        "  outdir = get_output_folder(output_path,batchdir)\n",
        "  save_settings = False #@param {type:\"boolean\"}\n",
        "  skip_save = True #@param {type:\"boolean\"}\n",
        "  skip_grid = True #@param {type:\"boolean\"}\n",
        "  display_grid = False #@param {type:\"boolean\"}\n",
        "  display_samples = True #@param {type:\"boolean\"}\n",
        "\n",
        "  #@markdown **Prompt Settings**\n",
        "  seed = 1138562047 #@param\n",
        "  prompt = \"futurism\" #@param {type:\"string\"}\n",
        "  from_file = False #@param {type:\"boolean\"}\n",
        "\n",
        "  #@markdown **Image Settings**\n",
        "  n_samples = 1 #@param\n",
        "  n_rows = 1 #@param\n",
        "  W = 256 #@param\n",
        "  H = 256 #@param\n",
        "\n",
        "  #@markdown **Init Settings**\n",
        "  use_init = False #@param {type:\"boolean\"}\n",
        "  init_image = \"/content/drive/MyDrive/AI/StableDiffusion/20220815180851_0.png\" #@param {type:\"string\"}\n",
        "  strength = 0.1 #@param {type:\"number\"}\n",
        "  \n",
        "  #@markdown **Sampling Settings**\n",
        "  scale = 10 #@param\n",
        "  n_iter = 1 #@param\n",
        "  ddim_steps = 500 #@param\n",
        "  ddim_eta = 0.0 #@param\n",
        "  plms = True #@param {type:\"boolean\"}\n",
        "  \n",
        "  #@markdown **Batch Settings**\n",
        "  n_batch = 1 #@param\n",
        "\n",
        "  #@markdown **no idea what this does**\n",
        "  precision = 'autocast' #@param\n",
        "  fixed_code = True #@param\n",
        "  C = 4 #@param\n",
        "  f = 4 #@param\n",
        "\n",
        "  return locals()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxx8BzxjiaXg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown **Run**\n",
        "params = opt_params()\n",
        "params[\"filename\"] = None\n",
        "for ii in range(params[\"n_batch\"]):\n",
        "  num = params[\"n_batch\"]\n",
        "  print(f\"run {ii+1} of {num}\")\n",
        "  run(params)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "c442uQJ_gUgy",
        "ov3r4RD1tzsT"
      ],
      "name": "Deforum Stable Diffusion",
      "provenance": [],
      "private_outputs": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}